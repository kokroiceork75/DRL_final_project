# DRL_final_project
# PPO.py
PPO.py 是用於訓練機器人的強化學習程式，採用了 Proximal Policy Optimization (PPO) 算法。其主要功能包括：

- **參數設置**：設置強化學習的各種參數，如折扣因子、隨機種子等。
- **Actor-Critic 網絡**：包含 Actor 和 Critic 神經網絡，分別用於選擇動作和評估狀態價值。
- **PPO 算法實現**：實現了 PPO 算法，包括選擇動作、存儲經驗、更新網絡等功能。
- **主訓練循環**：包含與 ROS 仿真環境的交互，收集經驗數據，計算獎勵，並更新網絡參數。

# environment_stage_4_ppo.py
environment_stage_4_ppo.py 定義了與仿真環境交互的功能，包括：

- **環境初始化**：設置 ROS 節點和話題，初始化機器人位置和目標點。
- **狀態獲取**：通過激光雷達和里程計數據獲取機器人的當前狀態。
- **獎勵設計**：根據機器人的行為和環境狀態計算獎勵。
- **步驟執行**：執行機器人的動作，返回新的狀態和獎勵。
- **環境重置**：重置仿真環境，設置新的目標點。

# respawnGoal.py
respawnGoal.py 負責目標點的生成和重生，具體功能包括：

- **初始化**：設置模型路徑和目標點初始位置。
- **檢查模型**：檢查目標點模型是否存在於仿真環境中。
- **重生模型**：在仿真環境中生成新的目標點模型。
- **刪除模型**：刪除現有的目標點模型。
- **獲取目標點位置**：根據當前環境情況生成新的目標點位置，並在仿真環境中進行重生。

# model.sdf
SDF 文件定義了一個靜態的目標盒子模型，其大小為 0.5 x 0.5 x 0.001，使用灰色材質並帶有紅色環境光。模型的位置和姿態在文件中已經定義，適用於 Gazebo 仿真環境。
